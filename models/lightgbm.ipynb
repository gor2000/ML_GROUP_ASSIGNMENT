{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-19T10:50:23.698968500Z",
     "start_time": "2023-10-19T10:50:23.676180900Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from sklearn.metrics import roc_auc_score as metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "tr_tr_new = joblib.load('../joblib/tr_tr_encoded.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T09:52:23.966686400Z",
     "start_time": "2023-10-19T09:52:23.790681600Z"
    }
   },
   "id": "d4ddd761374639ef"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "te_tr_new = joblib.load('../joblib/te_tr_encoded.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T09:52:25.339907800Z",
     "start_time": "2023-10-19T09:52:25.188624Z"
    }
   },
   "id": "9f30852225ba6706"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "X = tr_tr_new.drop('isFraud', axis=1)\n",
    "y = tr_tr_new['isFraud']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T09:53:10.279511300Z",
     "start_time": "2023-10-19T09:53:10.110943300Z"
    }
   },
   "id": "75a71cb5736a73ba"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T10:48:46.099929500Z",
     "start_time": "2023-10-19T10:48:45.013454100Z"
    }
   },
   "id": "ba8277889ddc0ab9"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Base parameters\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'n_jobs': -1,\n",
    "    'tree_learner': 'serial',\n",
    "    'seed': 0,\n",
    "    'device_type': 'gpu',\n",
    "    'metric': 'auc',\n",
    "}\n",
    "\n",
    "# Parameters for grid search\n",
    "params_grid = {\n",
    "    'learning_rate': [0.05, 0.08],\n",
    "    'colsample_bytree': [0.5, 0.6],\n",
    "    'subsample': [0.7, 0.8],\n",
    "    'n_estimators': [2000, 3000],\n",
    "    'early_stopping_rounds': [100]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T09:58:13.503136100Z",
     "start_time": "2023-10-19T09:58:13.489108Z"
    }
   },
   "id": "49b56b65cac433c5"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "\n",
    "# Set up LightGBM classifier\n",
    "lgb_classifier = lgb.LGBMClassifier(**lgb_params)\n",
    "\n",
    "\n",
    "# Set up grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    lgb_classifier, \n",
    "    param_grid=params_grid, \n",
    "    scoring='roc_auc', \n",
    "    cv=5,  # number of cross-validation folds, adjust as needed\n",
    "    verbose=2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T10:50:26.199655100Z",
     "start_time": "2023-10-19T10:50:26.188650500Z"
    }
   },
   "id": "32ab9d3b30b9bf3e"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364656\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19815\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.030479 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312018\n",
      "[LightGBM] [Info] Start training from score -3.312018\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.962532\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.7; total time=  53.4s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13288, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19884\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027372 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035159 -> initscore=-3.312096\n",
      "[LightGBM] [Info] Start training from score -3.312096\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.961143\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.7; total time=  54.0s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19820\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.025886 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1997]\tvalid_0's auc: 0.961957\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.7; total time=  56.4s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19829\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026493 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.962644\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.7; total time=  55.7s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19813\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.028710 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.962548\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.7; total time=  54.8s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364656\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19815\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027440 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312018\n",
      "[LightGBM] [Info] Start training from score -3.312018\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.962535\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.8; total time=  54.8s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13288, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19884\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027167 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035159 -> initscore=-3.312096\n",
      "[LightGBM] [Info] Start training from score -3.312096\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1999]\tvalid_0's auc: 0.960976\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.8; total time=  54.9s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19820\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.028071 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.961975\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.8; total time=  55.0s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19829\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026525 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.961541\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.8; total time=  55.0s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19813\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026530 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1989]\tvalid_0's auc: 0.961626\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.8; total time=  52.7s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364656\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19815\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027254 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312018\n",
      "[LightGBM] [Info] Start training from score -3.312018\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2994]\tvalid_0's auc: 0.966343\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.7; total time= 1.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13288, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19884\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.028184 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035159 -> initscore=-3.312096\n",
      "[LightGBM] [Info] Start training from score -3.312096\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2996]\tvalid_0's auc: 0.964588\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.7; total time= 1.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19820\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.028262 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2999]\tvalid_0's auc: 0.965406\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.7; total time= 1.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19829\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.024682 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2996]\tvalid_0's auc: 0.96565\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.7; total time= 1.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19813\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026721 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2999]\tvalid_0's auc: 0.965209\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.7; total time= 1.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364656\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19815\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027915 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312018\n",
      "[LightGBM] [Info] Start training from score -3.312018\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2996]\tvalid_0's auc: 0.966346\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.8; total time= 1.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13288, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19884\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.025413 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035159 -> initscore=-3.312096\n",
      "[LightGBM] [Info] Start training from score -3.312096\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2999]\tvalid_0's auc: 0.964571\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.8; total time= 1.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19820\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027859 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2999]\tvalid_0's auc: 0.965407\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.8; total time= 1.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19829\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027358 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2996]\tvalid_0's auc: 0.96565\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.8; total time= 1.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19813\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.025582 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2999]\tvalid_0's auc: 0.96521\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.8; total time= 1.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364656\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19815\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.024494 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312018\n",
      "[LightGBM] [Info] Start training from score -3.312018\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.964812\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.7; total time=  53.6s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13288, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19884\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027088 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035159 -> initscore=-3.312096\n",
      "[LightGBM] [Info] Start training from score -3.312096\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.963955\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.7; total time=  52.4s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19820\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027609 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.965608\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.7; total time=  55.0s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19829\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.025952 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1996]\tvalid_0's auc: 0.964995\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.7; total time=  55.8s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19813\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027678 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1999]\tvalid_0's auc: 0.964857\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.7; total time=  57.4s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364656\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19815\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.029151 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312018\n",
      "[LightGBM] [Info] Start training from score -3.312018\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.964707\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.8; total time=  56.9s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13288, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19884\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.028558 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035159 -> initscore=-3.312096\n",
      "[LightGBM] [Info] Start training from score -3.312096\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.963954\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.8; total time= 1.0min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19820\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.028212 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.965174\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.8; total time=  59.4s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19829\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027795 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.965283\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.8; total time=  56.2s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19813\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027980 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1999]\tvalid_0's auc: 0.964767\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.8; total time=  58.0s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364656\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19815\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.031177 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312018\n",
      "[LightGBM] [Info] Start training from score -3.312018\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2990]\tvalid_0's auc: 0.967182\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.7; total time= 1.5min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13288, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19884\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.030822 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035159 -> initscore=-3.312096\n",
      "[LightGBM] [Info] Start training from score -3.312096\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\tvalid_0's auc: 0.966232\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.7; total time= 1.4min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19820\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027016 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2998]\tvalid_0's auc: 0.967533\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.7; total time= 1.5min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19829\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026977 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2991]\tvalid_0's auc: 0.967194\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.7; total time= 1.4min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19813\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027902 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2987]\tvalid_0's auc: 0.966703\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.7; total time= 1.4min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364656\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19815\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.029414 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312018\n",
      "[LightGBM] [Info] Start training from score -3.312018\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2996]\tvalid_0's auc: 0.96722\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.8; total time= 1.4min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13288, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19884\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027880 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035159 -> initscore=-3.312096\n",
      "[LightGBM] [Info] Start training from score -3.312096\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2901]\tvalid_0's auc: 0.966095\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.8; total time= 1.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19820\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027113 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2972]\tvalid_0's auc: 0.966873\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.8; total time= 1.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19829\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027774 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\tvalid_0's auc: 0.967204\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.8; total time= 1.4min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19813\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.029185 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2944]\tvalid_0's auc: 0.965962\n",
      "[CV] END colsample_bytree=0.5, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.8; total time= 1.4min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364656\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19815\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026334 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312018\n",
      "[LightGBM] [Info] Start training from score -3.312018\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1999]\tvalid_0's auc: 0.96179\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.7; total time=  51.8s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13288, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19884\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026623 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035159 -> initscore=-3.312096\n",
      "[LightGBM] [Info] Start training from score -3.312096\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1999]\tvalid_0's auc: 0.961444\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.7; total time=  55.1s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19820\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.032110 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1999]\tvalid_0's auc: 0.962257\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.7; total time=  52.5s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19829\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027616 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.962459\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.7; total time=  52.4s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19813\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026440 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.961521\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.7; total time=  56.4s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364656\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19815\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027490 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312018\n",
      "[LightGBM] [Info] Start training from score -3.312018\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1998]\tvalid_0's auc: 0.961428\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.8; total time=  53.0s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13288, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19884\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.028294 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035159 -> initscore=-3.312096\n",
      "[LightGBM] [Info] Start training from score -3.312096\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1999]\tvalid_0's auc: 0.961445\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.8; total time=  54.3s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19820\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.028659 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.962287\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.8; total time=  52.6s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19829\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026706 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.961828\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.8; total time=  52.4s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19813\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.029084 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.961521\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=2000, subsample=0.8; total time=  53.2s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364656\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19815\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027570 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312018\n",
      "[LightGBM] [Info] Start training from score -3.312018\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2998]\tvalid_0's auc: 0.965279\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.7; total time= 1.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13288, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19884\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026905 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035159 -> initscore=-3.312096\n",
      "[LightGBM] [Info] Start training from score -3.312096\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2999]\tvalid_0's auc: 0.964574\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.7; total time= 1.4min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19820\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026611 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2999]\tvalid_0's auc: 0.966336\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.7; total time= 1.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19829\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027828 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2997]\tvalid_0's auc: 0.965425\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.7; total time= 1.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19813\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026990 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2995]\tvalid_0's auc: 0.964915\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.7; total time= 1.2min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364656\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19815\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026936 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312018\n",
      "[LightGBM] [Info] Start training from score -3.312018\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\tvalid_0's auc: 0.964907\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.8; total time= 1.2min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13288, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19884\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026761 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035159 -> initscore=-3.312096\n",
      "[LightGBM] [Info] Start training from score -3.312096\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2999]\tvalid_0's auc: 0.964574\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.8; total time= 1.2min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19820\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027417 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2987]\tvalid_0's auc: 0.965358\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.8; total time= 1.2min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19829\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027369 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\tvalid_0's auc: 0.965575\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.8; total time= 1.2min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19813\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026017 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\tvalid_0's auc: 0.965177\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.05, n_estimators=3000, subsample=0.8; total time= 1.2min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364656\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19815\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027487 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312018\n",
      "[LightGBM] [Info] Start training from score -3.312018\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.964523\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.7; total time=  48.7s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13288, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19884\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026911 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035159 -> initscore=-3.312096\n",
      "[LightGBM] [Info] Start training from score -3.312096\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1997]\tvalid_0's auc: 0.964098\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.7; total time=  48.9s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19820\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026569 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1996]\tvalid_0's auc: 0.965212\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.7; total time=  48.5s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19829\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027100 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1999]\tvalid_0's auc: 0.965509\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.7; total time=  48.4s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19813\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.025253 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1997]\tvalid_0's auc: 0.964449\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.7; total time=  48.4s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364656\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19815\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027168 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312018\n",
      "[LightGBM] [Info] Start training from score -3.312018\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1999]\tvalid_0's auc: 0.964237\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.8; total time=  48.5s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13288, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19884\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.028150 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035159 -> initscore=-3.312096\n",
      "[LightGBM] [Info] Start training from score -3.312096\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.96436\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.8; total time=  48.3s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19820\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027719 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1996]\tvalid_0's auc: 0.96522\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.8; total time=  48.4s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19829\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.025326 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1965]\tvalid_0's auc: 0.96494\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.8; total time=  48.6s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19813\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027394 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.964076\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=2000, subsample=0.8; total time=  48.4s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364656\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19815\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027878 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312018\n",
      "[LightGBM] [Info] Start training from score -3.312018\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\tvalid_0's auc: 0.967312\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.7; total time= 1.2min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13288, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19884\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027088 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035159 -> initscore=-3.312096\n",
      "[LightGBM] [Info] Start training from score -3.312096\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2995]\tvalid_0's auc: 0.965765\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.7; total time= 1.2min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19820\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027040 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2289]\tvalid_0's auc: 0.965832\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.7; total time=  57.4s\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19829\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027257 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2979]\tvalid_0's auc: 0.967432\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.7; total time= 1.2min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19813\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.025977 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2828]\tvalid_0's auc: 0.966475\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.7; total time= 1.2min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364656\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19815\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026559 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312018\n",
      "[LightGBM] [Info] Start training from score -3.312018\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2995]\tvalid_0's auc: 0.966565\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.8; total time= 1.2min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13288, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19884\n",
      "[LightGBM] [Info] Number of data points in the train set: 377945, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027657 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035159 -> initscore=-3.312096\n",
      "[LightGBM] [Info] Start training from score -3.312096\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\tvalid_0's auc: 0.966077\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.8; total time= 1.2min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19820\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027576 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2985]\tvalid_0's auc: 0.967169\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.8; total time= 1.2min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19829\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.026863 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2954]\tvalid_0's auc: 0.966779\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.8; total time= 1.2min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 13289, number of negative: 364657\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19813\n",
      "[LightGBM] [Info] Number of data points in the train set: 377946, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (21.63 MB) transferred to GPU in 0.027166 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312021\n",
      "[LightGBM] [Info] Start training from score -3.312021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2699]\tvalid_0's auc: 0.966741\n",
      "[CV] END colsample_bytree=0.6, early_stopping_rounds=100, learning_rate=0.08, n_estimators=3000, subsample=0.8; total time= 1.1min\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 16611, number of negative: 455821\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19839\n",
      "[LightGBM] [Info] Number of data points in the train set: 472432, number of used features: 213\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 60 dense feature groups (27.03 MB) transferred to GPU in 0.031944 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035161 -> initscore=-3.312035\n",
      "[LightGBM] [Info] Start training from score -3.312035\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2990]\tvalid_0's auc: 0.970738\n",
      "Best parameters found:  {'colsample_bytree': 0.6, 'early_stopping_rounds': 100, 'learning_rate': 0.08, 'n_estimators': 3000, 'subsample': 0.8}\n",
      "Best AUC score:  0.9670832534986824\n"
     ]
    }
   ],
   "source": [
    "# Run the grid search\n",
    "grid_search.fit(X_train, y_train, eval_metric='auc', eval_set=[(X_valid, y_valid)])\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best parameters found: \", best_params)\n",
    "print(\"Best AUC score: \", best_score)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T12:18:31.443791900Z",
     "start_time": "2023-10-19T10:50:40.434497700Z"
    }
   },
   "id": "ff7aeba40038bb3b"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "['gridSearchLightgbm.joblib']"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'gridSearchLightgbm.joblib'\n",
    "joblib.dump(grid_search, filename)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T13:01:32.670326200Z",
     "start_time": "2023-10-19T13:01:32.477640900Z"
    }
   },
   "id": "9e3caf52b6813548"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T14:16:51.304178700Z",
     "start_time": "2023-10-19T14:16:51.290237900Z"
    }
   },
   "id": "3a98d4638aaf6663"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19841\n",
      "[LightGBM] [Info] Number of data points in the train set: 472432, number of used features: 214\n",
      "[LightGBM] [Info] Start training from score 0.035161\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l2: 0.00851498\tvalid_1's l2: 0.0128258\n"
     ]
    }
   ],
   "source": [
    "best_model = lgb.train(best_params,\n",
    "                  train_data,\n",
    "                  valid_sets=[train_data, valid_data])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T14:17:43.838740800Z",
     "start_time": "2023-10-19T14:16:52.445903400Z"
    }
   },
   "id": "8b3c47b9dde2ac16"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "<lightgbm.basic.Booster at 0x2a9dcc9eb50>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T14:17:46.371140400Z",
     "start_time": "2023-10-19T14:17:46.357623200Z"
    }
   },
   "id": "619f6c8549e8c816"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "pred_train_p = best_model.predict(X_train)\n",
    "pred_val_p = best_model.predict(X_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T14:18:03.566504100Z",
     "start_time": "2023-10-19T14:17:54.871628600Z"
    }
   },
   "id": "71f442dc3adc35f4"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.9534\n",
      "Metric train = 0.9864 - Metric val = 0.9534\n"
     ]
    }
   ],
   "source": [
    "# Compute AUC\n",
    "auc_train = metric(y_train, pred_train_p)\n",
    "auc_val = metric(y_valid, pred_val_p)\n",
    "# If you have a separate test set, uncomment the line below\n",
    "# auc_test = roc_auc_score(y_test, pred_test_p)\n",
    "\n",
    "print(f\"Validation AUC: {auc_val:.4f}\")\n",
    "print('Metric train = %.4f - Metric val = %.4f' % (auc_train, auc_val))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T14:06:47.328503300Z",
     "start_time": "2023-10-19T14:06:47.186343Z"
    }
   },
   "id": "62d0251809501dd0"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "['lightgbm_model[0.9534].joblib']"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'lightgbm_model[0.9534].joblib'\n",
    "joblib.dump(best_model, filename)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T14:18:17.870119300Z",
     "start_time": "2023-10-19T14:18:17.714981900Z"
    }
   },
   "id": "f0fa2abb2f9108a9"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "pred_te_tr_new_p = best_model.predict(te_tr_new)\n",
    "\n",
    "# Create the output DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    'TransactionID': te_tr_new.reset_index()['TransactionID'],\n",
    "    'isFraud': pred_te_tr_new_p\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_df.to_csv('predicted_fraud_lightgbm[5].csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T13:19:50.459471100Z"
    }
   },
   "id": "843808d337f61928"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
